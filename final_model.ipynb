{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rfy73cIcL_z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ANN_Challenge1"
      ],
      "metadata": {
        "id": "0wdAYQQzMAcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba3BhRnoTs9e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDb1pjrlWK9A"
      },
      "outputs": [],
      "source": [
        "# Load the dataset to be used for classification\n",
        "!unzip split_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBc_cj2rX4sE"
      },
      "outputs": [],
      "source": [
        "# apply data augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "aug_data_gen = ImageDataGenerator(rotation_range=15,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=True, \n",
        "                                        fill_mode='reflect',\n",
        "                                        rescale=1/255.)\n",
        "\n",
        "train_dataset = aug_data_gen.flow_from_directory(directory='split_dataset/training',\n",
        "                                                 target_size=(96,96),\n",
        "                                                 color_mode='rgb',\n",
        "                                                classes=None,\n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=8,\n",
        "                                               shuffle=True)\n",
        "\n",
        "validation_dataset = aug_data_gen.flow_from_directory(directory='split_dataset/validation',\n",
        "                                                 target_size=(96,96),\n",
        "                                                 color_mode='rgb',\n",
        "                                                classes=None, \n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=8,\n",
        "                                               shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWpjtIyiZe1l"
      },
      "outputs": [],
      "source": [
        "# inspect the target classes\n",
        "print(\"Assigned labels\")\n",
        "print(train_dataset.class_indices)\n",
        "print()\n",
        "print(\"Training Target classes\")\n",
        "unique, counts = np.unique(train_dataset.classes, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "print()\n",
        "print(\"Validation Target classes\")\n",
        "unique, counts = np.unique(validation_dataset.classes, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import DenseNet169 as supernet\n",
        "denseNet = tf.keras.applications.DenseNet169(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "# set the supernet as not trainable to perform Transfer Learning\n",
        "denseNet.trainable = False\n",
        "\n",
        "# build the entire network    \n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Resizing(256,256,interpolation=\"lanczos5\")) # resize the input image\n",
        "model.add(tf.keras.layers.RandomContrast(0.3)) # apply random contrast to the input image\n",
        "model.add(denseNet)\n",
        "model.add(tf.keras.layers.GlobalAvgPool2D())\n",
        "model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units=8, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')\n",
        "\n",
        "# fit the model to the data\n",
        "history = model.fit(\n",
        "    x = train_dataset,\n",
        "    batch_size = 32,\n",
        "    epochs = 200,\n",
        "    validation_data = validation_dataset,\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)]\n",
        ").history"
      ],
      "metadata": {
        "id": "b01XDvlb207v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune with first 6 layers frozen\n",
        "model.get_layer('densenet169').trainable = True\n",
        "for i, layer in enumerate(model.get_layer('densenet169').layers[:6]):\n",
        "  layer.trainable=False\n",
        "\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')\n",
        "\n",
        "history = model.fit(\n",
        "    x = train_dataset,\n",
        "    batch_size = 32,\n",
        "    epochs = 200,\n",
        "    validation_data = validation_dataset,\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True)]\n",
        ").history"
      ],
      "metadata": {
        "id": "kJQaYExGEW3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('SubmissionModel')"
      ],
      "metadata": {
        "id": "oICvkxniLjwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}